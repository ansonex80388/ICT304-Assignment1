{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype - Intelligent Home Care System\n",
    "\n",
    "Quick prototype to test both sub-systems before full implementation:\n",
    "- Fall Detection (LSTM vs GRU)\n",
    "- ECG Anomaly Detection (LSTM Autoencoder vs GRU Autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.20.0\n",
      "SisFall found: True\n",
      "ECG found: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, RepeatVector, TimeDistributed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "SISFALL_PATH = Path(r'C:\\Users\\MYP\\Desktop\\Parthiban\\Uni\\CS_AI\\2026_Term_4\\ICT304\\Assignment\\Assignment 1\\datasets\\SisFall_dataset')\n",
    "ECG_PATH = Path(r'C:\\Users\\MYP\\Desktop\\Parthiban\\Uni\\CS_AI\\2026_Term_4\\ICT304\\Assignment\\Assignment 1\\datasets\\Heartbeat')\n",
    "\n",
    "print(f'TF version: {tf.__version__}')\n",
    "print(f'SisFall found: {SISFALL_PATH.exists()}')\n",
    "print(f'ECG found: {ECG_PATH.exists()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Fall Detection (LSTM vs GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to parse sisfall files (comma separated, semicolon at end of each line)\n",
    "def load_sisfall_file(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        line = line.strip().rstrip(';')\n",
    "        if line:\n",
    "            vals = [int(x.strip()) for x in line.split(',')]\n",
    "            data.append(vals)\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "falls: 45, adl: 45\n"
     ]
    }
   ],
   "source": [
    "# loading a small subset for the prototype (3 subjects, 15 files each for falls/ADLs)\n",
    "X_fall, X_adl = [], []\n",
    "\n",
    "for subj in ['SA01', 'SA02', 'SA03']:\n",
    "    subj_path = SISFALL_PATH / subj\n",
    "    all_files = list(subj_path.glob('*.txt'))\n",
    "\n",
    "    fall_files = [f for f in all_files if f.name.startswith('F')][:15]\n",
    "    adl_files = [f for f in all_files if f.name.startswith('D')][:15]\n",
    "\n",
    "    for file in fall_files + adl_files:\n",
    "        try:\n",
    "            data = load_sisfall_file(file)\n",
    "            mid = len(data) // 2\n",
    "            window = data[mid-100:mid+100] if len(data) > 200 else data[:200]\n",
    "            if len(window) == 200:\n",
    "                if file.name.startswith('F'):\n",
    "                    X_fall.append(window)\n",
    "                else:\n",
    "                    X_adl.append(window)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(f'falls: {len(X_fall)}, adl: {len(X_adl)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (72, 200, 9), test: (18, 200, 9)\n"
     ]
    }
   ],
   "source": [
    "# balance + normalize + split\n",
    "n = min(len(X_fall), len(X_adl))\n",
    "X = np.array(X_fall[:n] + X_adl[:n])\n",
    "y = np.array([1]*n + [0]*n)  # 1=fall, 0=adl\n",
    "\n",
    "idx = np.random.permutation(len(X))\n",
    "X, y = X[idx], y[idx]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X.reshape(-1, 9)).reshape(X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.2, random_state=42)\n",
    "print(f'train: {X_train.shape}, test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.7368 - loss: 0.6283 - val_accuracy: 0.6667 - val_loss: 0.6673\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7368 - loss: 0.5850 - val_accuracy: 0.6667 - val_loss: 0.6648\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7544 - loss: 0.5502 - val_accuracy: 0.6667 - val_loss: 0.6639\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7719 - loss: 0.5214 - val_accuracy: 0.6667 - val_loss: 0.6647\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7895 - loss: 0.4975 - val_accuracy: 0.6667 - val_loss: 0.6668\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7895 - loss: 0.4774 - val_accuracy: 0.6667 - val_loss: 0.6697\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8070 - loss: 0.4599 - val_accuracy: 0.6667 - val_loss: 0.6727\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7895 - loss: 0.4440 - val_accuracy: 0.6667 - val_loss: 0.6751\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8070 - loss: 0.4287 - val_accuracy: 0.6667 - val_loss: 0.6762\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8070 - loss: 0.4137 - val_accuracy: 0.6667 - val_loss: 0.6755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19514f0b2c0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM\n",
    "lstm = Sequential([\n",
    "    LSTM(32, input_shape=(200, 9)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.5965 - loss: 0.6805 - val_accuracy: 0.6000 - val_loss: 0.6780\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7368 - loss: 0.6448 - val_accuracy: 0.5333 - val_loss: 0.6759\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.7719 - loss: 0.6145 - val_accuracy: 0.5333 - val_loss: 0.6740\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7368 - loss: 0.5879 - val_accuracy: 0.5333 - val_loss: 0.6722\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7368 - loss: 0.5648 - val_accuracy: 0.5333 - val_loss: 0.6706\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.7544 - loss: 0.5446 - val_accuracy: 0.5333 - val_loss: 0.6690\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.7895 - loss: 0.5269 - val_accuracy: 0.5333 - val_loss: 0.6676\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7895 - loss: 0.5115 - val_accuracy: 0.5333 - val_loss: 0.6662\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7895 - loss: 0.4979 - val_accuracy: 0.5333 - val_loss: 0.6651\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7895 - loss: 0.4858 - val_accuracy: 0.6000 - val_loss: 0.6642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1950a5ed640>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRU\n",
    "gru = Sequential([\n",
    "    GRU(32, input_shape=(200, 9)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "gru.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "gru.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall Detection Results\n",
      "  LSTM -> acc: 0.7222, f1: 0.6154\n",
      "  GRU  -> acc: 0.7222, f1: 0.6154\n"
     ]
    }
   ],
   "source": [
    "# results\n",
    "lstm_pred = (lstm.predict(X_test, verbose=0) > 0.5).astype(int).flatten()\n",
    "gru_pred = (gru.predict(X_test, verbose=0) > 0.5).astype(int).flatten()\n",
    "\n",
    "print('Fall Detection Results')\n",
    "print(f'  LSTM -> acc: {accuracy_score(y_test, lstm_pred):.4f}, f1: {f1_score(y_test, lstm_pred):.4f}')\n",
    "print(f'  GRU  -> acc: {accuracy_score(y_test, gru_pred):.4f}, f1: {f1_score(y_test, gru_pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - ECG Anomaly Detection (LSTM AE vs GRU AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecg shape: (5000, 187)\n",
      "normal: 4180, anomaly: 820\n"
     ]
    }
   ],
   "source": [
    "# load ecg data - randomly sample 5000 (the csv is sorted by class)\n",
    "train_df = pd.read_csv(ECG_PATH / 'mitbih_train.csv', header=None)\n",
    "train_df = train_df.sample(n=5000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "X_ecg = train_df.iloc[:, :-1].values\n",
    "y_ecg = train_df.iloc[:, -1].values\n",
    "y_binary = (y_ecg != 0).astype(int)  # 0=normal, 1=anomaly\n",
    "\n",
    "print(f'ecg shape: {X_ecg.shape}')\n",
    "print(f'normal: {sum(y_binary==0)}, anomaly: {sum(y_binary==1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on 3344 normal heartbeats\n"
     ]
    }
   ],
   "source": [
    "# autoencoders are trained only on normal heartbeats\n",
    "# the idea: if it only learns \"normal\", anomalies will have higher reconstruction error\n",
    "X_normal = X_ecg[y_binary == 0].reshape(-1, 187, 1)\n",
    "\n",
    "X_train_ae, X_val_ae = train_test_split(X_normal, test_size=0.2, random_state=42)\n",
    "print(f'training on {len(X_train_ae)} normal heartbeats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - loss: 0.0458 - val_loss: 0.0432\n",
      "Epoch 2/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 85ms/step - loss: 0.0401 - val_loss: 0.0380\n",
      "Epoch 3/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 91ms/step - loss: 0.0321 - val_loss: 0.0280\n",
      "Epoch 4/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 89ms/step - loss: 0.0274 - val_loss: 0.0257\n",
      "Epoch 5/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 88ms/step - loss: 0.0248 - val_loss: 0.0250\n",
      "Epoch 6/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 85ms/step - loss: 0.0243 - val_loss: 0.0237\n",
      "Epoch 7/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 94ms/step - loss: 0.0235 - val_loss: 0.0232\n",
      "Epoch 8/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 88ms/step - loss: 0.0220 - val_loss: 0.0189\n",
      "Epoch 9/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 87ms/step - loss: 0.0184 - val_loss: 0.0169\n",
      "Epoch 10/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 87ms/step - loss: 0.0167 - val_loss: 0.0157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1950aa2ca70>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM autoencoder\n",
    "# using tanh instead of relu to avoid the NaN issue with relu on autoencoders\n",
    "lstm_ae = Sequential([\n",
    "    LSTM(32, activation='tanh', input_shape=(187, 1), return_sequences=False),\n",
    "    RepeatVector(187),\n",
    "    LSTM(32, activation='tanh', return_sequences=True),\n",
    "    TimeDistributed(Dense(1))\n",
    "])\n",
    "lstm_ae.compile(optimizer='adam', loss='mse')\n",
    "lstm_ae.fit(X_train_ae, X_train_ae, epochs=10, batch_size=32, validation_data=(X_val_ae, X_val_ae), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 131ms/step - loss: 0.0496 - val_loss: 0.0484\n",
      "Epoch 2/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 112ms/step - loss: 0.0451 - val_loss: 0.0435\n",
      "Epoch 3/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 97ms/step - loss: 0.0395 - val_loss: 0.0337\n",
      "Epoch 4/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 89ms/step - loss: 0.0320 - val_loss: 0.0292\n",
      "Epoch 5/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 101ms/step - loss: 0.0236 - val_loss: 0.0214\n",
      "Epoch 6/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 99ms/step - loss: 0.0197 - val_loss: 0.0192\n",
      "Epoch 7/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 115ms/step - loss: 0.0177 - val_loss: 0.0176\n",
      "Epoch 8/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 100ms/step - loss: 0.0172 - val_loss: 0.0171\n",
      "Epoch 9/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 96ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 10/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 91ms/step - loss: 0.0165 - val_loss: 0.0166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1950a689be0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRU autoencoder\n",
    "gru_ae = Sequential([\n",
    "    GRU(32, activation='tanh', input_shape=(187, 1), return_sequences=False),\n",
    "    RepeatVector(187),\n",
    "    GRU(32, activation='tanh', return_sequences=True),\n",
    "    TimeDistributed(Dense(1))\n",
    "])\n",
    "gru_ae.compile(optimizer='adam', loss='mse')\n",
    "gru_ae.fit(X_train_ae, X_train_ae, epochs=10, batch_size=32, validation_data=(X_val_ae, X_val_ae), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 160 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000195937AE660> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "ECG Anomaly Detection Results\n",
      "  LSTM AE -> acc: 0.8348, f1: 0.3295\n",
      "  GRU AE  -> acc: 0.8142, f1: 0.1771\n"
     ]
    }
   ],
   "source": [
    "# anomaly detection via reconstruction error\n",
    "X_all = X_ecg.reshape(-1, 187, 1)\n",
    "\n",
    "lstm_recon = lstm_ae.predict(X_all, verbose=0)\n",
    "gru_recon = gru_ae.predict(X_all, verbose=0)\n",
    "\n",
    "lstm_error = np.mean((X_all - lstm_recon)**2, axis=(1, 2))\n",
    "gru_error = np.mean((X_all - gru_recon)**2, axis=(1, 2))\n",
    "\n",
    "# threshold = 95th percentile of reconstruction error on normal samples\n",
    "lstm_thresh = np.percentile(lstm_error[y_binary == 0], 95)\n",
    "gru_thresh = np.percentile(gru_error[y_binary == 0], 95)\n",
    "\n",
    "lstm_pred_ae = (lstm_error > lstm_thresh).astype(int)\n",
    "gru_pred_ae = (gru_error > gru_thresh).astype(int)\n",
    "\n",
    "print('ECG Anomaly Detection Results')\n",
    "print(f'  LSTM AE -> acc: {accuracy_score(y_binary, lstm_pred_ae):.4f}, f1: {f1_score(y_binary, lstm_pred_ae):.4f}')\n",
    "print(f'  GRU AE  -> acc: {accuracy_score(y_binary, gru_pred_ae):.4f}, f1: {f1_score(y_binary, gru_pred_ae):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype Results\n",
      "\n",
      "Fall Detection:\n",
      "  LSTM  72.2% acc\n",
      "  GRU   72.2% acc\n",
      "\n",
      "ECG Anomaly Detection:\n",
      "  LSTM AE  83.5% acc\n",
      "  GRU AE   81.4% acc\n",
      "\n",
      "this is just a small prototype - full version will use the complete dataset with tuning\n"
     ]
    }
   ],
   "source": [
    "print('Prototype Results')\n",
    "print()\n",
    "print('Fall Detection:')\n",
    "print(f'  LSTM  {accuracy_score(y_test, lstm_pred)*100:.1f}% acc')\n",
    "print(f'  GRU   {accuracy_score(y_test, gru_pred)*100:.1f}% acc')\n",
    "print()\n",
    "print('ECG Anomaly Detection:')\n",
    "print(f'  LSTM AE  {accuracy_score(y_binary, lstm_pred_ae)*100:.1f}% acc')\n",
    "print(f'  GRU AE   {accuracy_score(y_binary, gru_pred_ae)*100:.1f}% acc')\n",
    "print()\n",
    "print('this is just a small prototype - full version will use the complete dataset with tuning')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
